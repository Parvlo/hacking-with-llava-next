{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHoBNX_Vff7V"
      },
      "source": [
        "# Hacking with [Llava-Next](https://llava-vl.github.io/blog/2024-01-30-llava-next/)!\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/hacking-with-llava-next/blob/main/notebooks/Llava_Next_on_TextVQA.ipynb)\n",
        "\n",
        "This notebook was created by [Harpreet Sahota](https://twitter.com/DataScienceHarp), Hacker-in-Residence at [Voxel 51](https://voxel51.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/harpreetsahota204/hacking-with-llava-next.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AwWcDxO6AcI"
      },
      "outputs": [],
      "source": [
        "!pip install -r \"content/hacking-with-llava-next/requirements.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcOYnk0qcPSn"
      },
      "source": [
        "Let's start by loading the model and processors from the Hugging Face Hub.\n",
        "\n",
        "We'll try out [llava-v1.6-mistral-7b-hf](https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf) and [llava-v1.6-mistral-7b-hf](https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf) varieties of these models.\n",
        "\n",
        "Note that this notebook is run on an A100 from Google Colab Pro+, though you can use a V100 as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"/content/hacking-with-llava-next/src\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLXLPN1l6PMD"
      },
      "outputs": [],
      "source": [
        "from utils import load_model_and_processor\n",
        "\n",
        "mistral_llava, mistral_llava_processor = load_model_and_processor(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
        "\n",
        "vicuna_llava, vicuna_llava_processor = load_model_and_processor(\"llava-hf/llava-v1.6-vicuna-7b-hf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from prompts import MISTRAL_PROMPT, VICUNA_PROMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXURkK6evwYF"
      },
      "outputs": [],
      "source": [
        "model_dict = {\n",
        "    \"Mistral\": {\n",
        "        \"prompt_template\": MISTRAL_PROMPT,\n",
        "        \"model\": mistral_llava,\n",
        "        \"processor\": mistral_llava_processor\n",
        "    },\n",
        "    \"Vicuna\": {\n",
        "        \"prompt_template\": VICUNA_PROMPT,\n",
        "        \"model\": vicuna_llava,\n",
        "        \"processor\": vicuna_llava_processor\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU3_00UwBxcb"
      },
      "source": [
        "# Let's test the models on the following image!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pKo_hR0Br1k"
      },
      "outputs": [],
      "source": [
        "from utils import ask_question_of_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "qicQ0k3p1Hb3",
        "outputId": "90b62e70-8074-4734-e376-9a02fb0d2cd0"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "image = Image.open(\"\")\n",
        "\n",
        "questions = [\n",
        "    \"\",\n",
        "    \"\",\n",
        "    \"\",\n",
        "    \"\",\n",
        "    ]\n",
        "\n",
        "# Loop through each model and question\n",
        "for model_name, details in model_dict.items():\n",
        "    for question in questions:\n",
        "        # Markdown formatted print for model and question\n",
        "        display(Markdown(f\"### Model: **{model_name}**, Question: **{question}**\"))\n",
        "\n",
        "        # Call your function with the current parameters\n",
        "        ask_question_of_image(\n",
        "            image=image,\n",
        "            prompt_template=details[\"prompt_template\"],\n",
        "            question=question,\n",
        "            model=details[\"model\"],\n",
        "            processor=details[\"processor\"]\n",
        "        )\n",
        "\n",
        "        # Print a horizontal rule in Markdown for separation\n",
        "        display(Markdown(\"---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr08vc3PehQh"
      },
      "source": [
        "Now, let's test the models on a larger dataset.\n",
        "\n",
        "Let's download an oldie, but a goodie, [the TextVQA dataset](https://huggingface.co/datasets/textvqa) from Hugging Face. We'll make use of the validation set since we want some answers for evaluation.\n",
        "\n",
        "To save time, let's just take a small subset of the entire validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTQ4siJ6EqQh"
      },
      "outputs": [],
      "source": [
        "from utils import prepare_dataset\n",
        "\n",
        "textvqa_val_subset = prepare_dataset(\n",
        "    dataset_name=\"textvqa\", \n",
        "    split=\"validation\", \n",
        "    num_samples=500\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAFHlH9NfE_K"
      },
      "source": [
        "## And now we can run inference!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKr9iZF4QKsn"
      },
      "outputs": [],
      "source": [
        "from utils import run_inference_on_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghGue9UJQvya"
      },
      "outputs": [],
      "source": [
        "textvqa_val_subset = run_inference_on_dataset(\n",
        "    dataset=textvqa_val_subset,\n",
        "    prompt_template=model_dict[\"Mistral\"][\"prompt_template\"],\n",
        "    output_key=\"mistral_answer\",\n",
        "    model=model_dict[\"Mistral\"][\"model\"],\n",
        "    processor=model_dict[\"Mistral\"][\"processor\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPQGf8iITvwF"
      },
      "outputs": [],
      "source": [
        "textvqa_val_subset = run_inference_on_dataset(\n",
        "    dataset=textvqa_val_subset,\n",
        "    prompt_template=model_dict[\"Vicuna\"][\"prompt_template\"],\n",
        "    output_key=\"vicuna_answer\",\n",
        "    model=model_dict[\"Vicuna\"][\"model\"],\n",
        "    processor=model_dict[\"Vicuna\"][\"processor\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q-5cmlGfOe-"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "The authors of Llava-Next forked ElutherAI's evaluation harness and built on top of it. That project is called [The Evaluation Suite of Large Multimodal Models](https://github.com/EvolvingLMMs-Lab/lmms-eval/tree/main). It's a bit hacky at the moment, but I think it's a step in the right direction.\n",
        "\n",
        "I'm [adapting the code](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/lmms_eval/tasks/textvqa/utils.py) that the authors used for evaluation to better suit our setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "nnccqSkItJc1"
      },
      "outputs": [],
      "source": [
        "from utils import add_accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN8SKd_0PY-I"
      },
      "outputs": [],
      "source": [
        "columns_to_evaluate = [\"mistral_answer\", \"vicuna_answer\"]\n",
        "textvqa_val_subset = textvqa_val_subset.map(add_accuracy_score, fn_kwargs={\"columns_to_evaluate\": columns_to_evaluate})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qZc0Y81inSP"
      },
      "source": [
        "Let's take a quick look at the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "7LJnceMGhFCZ",
        "outputId": "b192b257-4a34-4e3c-a38b-6069677fc017"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = textvqa_val_subset.to_pandas()\n",
        "\n",
        "means = df[['mistral_answer_score', 'vicuna_answer_score']].mean()\n",
        "std_errs = df[['mistral_answer_score', 'vicuna_answer_score']].sem()\n",
        "error = std_errs\n",
        "\n",
        "# Creating the bar plot\n",
        "fig, ax = plt.subplots()\n",
        "means.plot(kind='bar', yerr=error, capsize=4, ax=ax, color=['#1f77b4', '#ff7f0e'], rot=0)\n",
        "\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Average Scores with Error Bars')\n",
        "ax.set_xticklabels(['Mistral Answer Score', 'Vicuna Answer Score'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nW03AdhgTit"
      },
      "source": [
        "# Visualzing results in `fiftyone`!\n",
        "\n",
        "It's definitley a close call for both models! But, looking at aggregrate metrics doesn't tell the whole story. It's not satisifying enough.\n",
        "\n",
        "That's where `fiftyone` comes in.\n",
        "\n",
        "Now that we've run inference and evaluation, let's massage our dataset into [`fiftyone`](https://github.com/voxel51/fiftyone) format so that we can visualize it in the `fiftyone` app.\n",
        "\n",
        "In the app we can easily visually inspect the behavior of our models. We can see where the models agree, where they disagree, and where they differ from the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0JdqHn0cnx-"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "import datasets\n",
        "import os\n",
        "import PIL\n",
        "\n",
        "def _get_extension(image):\n",
        "    if isinstance(image, PIL.PngImagePlugin.PngImageFile):\n",
        "        return \".png\"\n",
        "    elif isinstance(image, PIL.JpegImagePlugin.JpegImageFile):\n",
        "        return \".jpg\"\n",
        "    else:\n",
        "        return \"web\"\n",
        "\n",
        "def load_textvqa_dataset_in_fiftyone(\n",
        "        hf_dataset=textvqa_val_subset,\n",
        "        download_dir='/content/textvqa_subset',\n",
        "        name=\"textvqa\"):\n",
        "\n",
        "    dataset = fo.Dataset(name=name, persistent=True, overwrite=True)\n",
        "\n",
        "    samples = []\n",
        "    for i, item in enumerate(hf_dataset):\n",
        "        img = item['image']\n",
        "        ext = _get_extension(img)\n",
        "        fp = os.path.join(download_dir, f'{i}{ext}')\n",
        "        if not os.path.exists(fp):\n",
        "            img.save(fp)\n",
        "\n",
        "        sample_dict = {\n",
        "        \"filepath\": fp,\n",
        "        \"tags\": item['image_classes'],\n",
        "        \"question\": item['question'],\n",
        "        \"acceptable_answers\": list(set(item['answers'])),\n",
        "        \"vicuna_answer\": item['vicuna_answer'],\n",
        "        'mistral_answer': item['mistral_answer'],\n",
        "        'mistral_answer_score': item['mistral_answer_score'],\n",
        "        'vicuna_answer_score': item['vicuna_answer_score'],\n",
        "        'image_classes': item['image_classes'],\n",
        "        }\n",
        "\n",
        "        sample = fo.Sample(**sample_dict)\n",
        "        samples.append(sample)\n",
        "\n",
        "    dataset.add_samples(samples)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhX7YCK0gQ-3"
      },
      "outputs": [],
      "source": [
        "textvqa_test_fo = load_textvqa_dataset_in_fiftyone()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6iVjh2Egyfy"
      },
      "source": [
        "With our data in `fiftyone` format, we can visually inspect how the models perform.\n",
        "\n",
        "Notice on the side panel you can filter to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        },
        "id": "_Ee2Mb6MnJQm",
        "outputId": "d36b2349-1df1-4297-e799-8f9d0f40b16e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "\n",
              "@import url(\"https://fonts.googleapis.com/css2?family=Palanquin&display=swap\");\n",
              "\n",
              "body, html {\n",
              "  margin: 0;\n",
              "  padding: 0;\n",
              "  width: 100%;\n",
              "}\n",
              "\n",
              "#focontainer-521edb0e-328d-4e44-b912-225f1ca1d20e {\n",
              "  position: relative;\n",
              "  height: px;\n",
              "  display: block !important;\n",
              "}\n",
              "#foactivate-521edb0e-328d-4e44-b912-225f1ca1d20e {\n",
              "  font-weight: bold;\n",
              "  cursor: pointer;\n",
              "  font-size: 24px;\n",
              "  border-radius: 3px;\n",
              "  text-align: center;\n",
              "  padding: 0.5em;\n",
              "  color: rgb(255, 255, 255);\n",
              "  font-family: \"Palanquin\", sans-serif;\n",
              "  position: absolute;\n",
              "  left: 50%;\n",
              "  top: 50%;\n",
              "  width: 160px;\n",
              "  margin-left: -80px;\n",
              "  margin-top: -23px;\n",
              "  background: hsla(210,11%,15%, 0.8);\n",
              "  border: none;\n",
              "}\n",
              "#foactivate-521edb0e-328d-4e44-b912-225f1ca1d20e:focus {\n",
              "  outline: none;\n",
              "}\n",
              "#fooverlay-521edb0e-328d-4e44-b912-225f1ca1d20e {\n",
              "  width: 100%;\n",
              "  height: 100%;\n",
              "  background: hsla(208, 7%, 46%, 0.7);\n",
              "  position: absolute;\n",
              "  top: 0;\n",
              "  left: 0;\n",
              "  display: none;\n",
              "  cursor: pointer;\n",
              "}\n",
              "</style>\n",
              "<div id=\"focontainer-521edb0e-328d-4e44-b912-225f1ca1d20e\" style=\"display: none;\">\n",
              "   <div id=\"fooverlay-521edb0e-328d-4e44-b912-225f1ca1d20e\">\n",
              "      <button id=\"foactivate-521edb0e-328d-4e44-b912-225f1ca1d20e\" >Activate</button>\n",
              "   </div>\n",
              "</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "session = fo.launch_app(textvqa_test_fo)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
